# -*- coding: utf-8 -*-
"""HeartDiseaseDNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IvoGt3WeEfFcmX6Es_s8MDeugrfbXxac
"""

import tensorflow as tf
import pandas as pd
import numpy as np
from google.colab import drive
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
drive.mount('/content/drive')

path = '/content/drive/MyDrive/Heart_Disease_Prediction.csv'
Db = pd.read_csv(path)
Db.head()

print(Db.isnull().sum())
Db['Heart Disease']=Db['Heart Disease'].map({'Presence':1,'Absence':0})

X = Db.drop('Heart Disease',axis=1)
Y = Db['Heart Disease']
X_train , X_test , Y_train , Y_test = train_test_split(X,Y, test_size=0.2 , random_state = 43)
X_train_ , X_train_val , Y_train_ , Y_train_val = train_test_split(X_train , Y_train,test_size=0.15,random_state=43)
scaler = RobustScaler()
X_train_ = scaler.fit_transform(X_train_)
X_train_val = scaler.transform(X_train_val)
X_test = scaler.transform(X_test)

model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(100,activation='leaky_relu'),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(64,activation='leaky_relu'),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(24,activation='leaky_relu'),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(8,activation='leaky_relu'),
    tf.keras.layers.Dense(1,activation='sigmoid')
])
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0003), loss='binary_crossentropy', metrics=['accuracy'])
history = model.fit(X_train_, Y_train_, epochs=20,batch_size=16, validation_data=(X_train_val, Y_train_val))
model.evaluate(X_test, Y_test, verbose=2)

"""
import seaborn as sns
plt.figure(figsize=(4,8))
corr = Db.corr()[['Heart Disease']].sort_values(by='Heart Disease', ascending=False)
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Feature Correlation with Target')
plt.show()

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'],label='train_accuracy')
plt.plot(history.history['val_accuracy'],label='val_accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns
y_pred_probs = model.predict(X_test)
y_pred = (y_pred_probs > 0.5).astype(int)
cm = confusion_matrix(Y_test, y_pred)

plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Healthy', 'Sick'],
            yticklabels=['Healthy', 'Sick'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Heart Disease Confusion Matrix')
plt.show()

import numpy as np
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import seaborn as sns

def get_manual_permutation_importance(model, X, y, scaler, feature_names):
    # 1. Get the baseline Recall
    # We assume X is already scaled here because it's the test set
    y_pred_probs = model.predict(X, verbose=0)
    y_pred = (y_pred_probs > 0.5).astype(int)
    baseline_accuracy = accuracy_score(y, y_pred)

    importances = []

    # 2. Loop through each column by index
    for i in range(len(feature_names)):
        X_temp = X.copy()

        # 3. SCRAMBLE the specific column (index i)
        # We shuffle the values within that one column
        # Shuffling 20 times to get a stable "Truth"
        for _ in range(20):
            X_temp[:, i] = np.random.permutation(X_temp[:, i])


        # 4. Predict again with the "broken" data
        y_scrambled_probs = model.predict(X_temp, verbose=0)
        y_scrambled_pred = (y_scrambled_probs > 0.5).astype(int)

        # 5. Measure the drop in Recall
        scrambled_accuracy = accuracy_score(y, y_scrambled_pred)
        importances.append(baseline_accuracy - scrambled_accuracy)

    return np.array(importances)

# 3. RUN IT
# Make sure X_test_scaled is your scaled numpy array and original_df.columns is your list of names
names = Db.drop('Heart Disease', axis=1).columns # Or wherever you kept your feature names

importance = get_manual_permutation_importance(model, X_test, Y_test, scaler, names)

# 4. PLOT
plt.figure(figsize=(10,6))
sns.barplot(x=importance, y=names)
plt.title('Feature Importance: Impact on Accuracy')
plt.xlabel('Decrease in Accuracy when feature is scrambled')
plt.show()

from sklearn.inspection import permutation_importance
from scikeras.wrappers import KerasClassifier
wrapped_model = KerasClassifier(model=model, check_params=False)
wrapped_model.classes_ = np.array([0, 1])
results = permutation_importance(wrapped_model, X_test, Y_test, scoring='recall')
importance = results.importances_mean
feature_names = X.columns

plt.figure(figsize=(10,6))
sns.barplot(x=importance, y=feature_names)
plt.title('Which Features Drive the Prediction?')
plt.xlabel('Decrease in Recal when feature is removed')
plt.show()


def predict_heart_disease(user_features):
  raw_data = np.array(user_features).reshape(1, -1)
  scaled_data = scaler.transform(raw_data)
  prediction = model.predict(scaled_data, verbose=0)
  probability = prediction[0][0]
  if probability > 0.5:
        return f"Warning: Positive for Disease ({probability*100:.2f}% probability)"
  else:
        return f"Results: Negative for Disease ({probability*100:.2f}% probability)"

print("Enter your details")
l=[]
for i in range(13):
  a = input()
  l.append(a)
print(predict_heart_disease(l))
"""
